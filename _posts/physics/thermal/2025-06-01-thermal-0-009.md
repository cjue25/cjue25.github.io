---
title:  "(9) 熱力 ─ Thermal & Information(Shannon) Entropy"
categories:
  - physics
tags:
  - thermal
---


這是普通熱力學最後的章節，我們來把前面Thermal和Information(Shannon) Entropy的概念連結在一起。


## 複習

### Information Entropy (Shannon)


對於一個資訊量很大、自由度很高的系統，我們放棄去了解每一個粒子的運動，而是引進統計的概念，對想知道的系統變量定出 Random Variable，以及描述其的機率分布，接著便可進一步界定量化此系統不確定的方式(以銅版為例，正反面就是Random Variable，機率分布是Bernouli Distribution，不確定性在於你每擲一次，不知道會出現正面還是反面的結果)，這個不確定性指標就是Shannon Entropy

$$
\sigma \equiv \langle -\ln P \rangle = - \Sigma_s - P_s \ln P_s
$$

其中s為所有可能的state。


這個數值越大，代表系統的不確定性越大，「亂度」也越大。


而從之前簡單的Dog-Flea Model知道，此亂度會隨著時間逐漸上升到最大值之後維持平衡。


### Thermal Entropy

而在熱力學中，我們就是畫個P-V圖，定義出熵

$$
S(B)-S(A) = \int_A^B \frac{dQ}{T}
$$

如果是可逆的，積一圈回來就是0

{% include posts-fig.html path="/physics_thermal/Fig9-1.png" %}

$$
\oint \frac{dQ}{T} = 0
$$


跟potential的概念很像，和路徑無關，只和端點狀態有關，所以Thermal Entropy是狀態函數。


微分形式

$$
TdS=dQ
$$


### Relation


有一個關係式可以連結這兩種Entropy

$$
S=k\sigma
$$

其中k就是Boltzman constant。


接下來我們要從兩個角度證明這個公式
1. 從熱平衡波茲曼分布出發
2. 更General的從能量出發


## 證明一：波茲曼分布


假設現在有一個系統，和熱庫接觸並交換能量

{% include posts-fig.html path="/physics_thermal/Fig9-2.png" %}


一旦在交換能量了，這個系統的能量就不再是定值，是Random Variable，必須用機率分布去描述。


假設達到熱平衡時狀態出現的機率分布滿足波茲曼分布

$$
P_s\propto e^{\frac{-E_s}{kT}}
$$

> 波茲曼分布我們知道能量越低機率越高，所以可以想像就是這個系統一和熱庫接觸就會想偷懶XD


成正比我們用個正比常數

$$
P_s=\frac{1}{Z}e^{\frac{-E_s}{kT}}
$$


已知所有狀態機率總和為1

$$
\Sigma_s P_s = 1
$$

所以

$$
\Sigma_s \frac{1}{Z}e^{\frac{-E_s}{kT}} = 1\\
\Rightarrow Z = \Sigma_s e^{\frac{-E_s}{kT}}
$$

這個$Z$稱為partition function分割函數。(其實就很像我們切個大餅，切1塊、2塊、4塊，那就是總共分成7塊。)


### 套用理想氣體

接著把這個套用到理想氣體上，但首先我們要先定義什麼是「狀態」？


狀態就是所有可能的狀態，想知道一個粒子在幹嘛，就要知道他在哪，會有多快。


因此在三維的世界裡，在哪$\vec{r}$，就要用$(x,y,z)$描述，多快$\vec{p}$也是用動量$(p_x,p_y,p_z)$，所以機率分布的變數有兩個，各自有三個分量

$$
P(\vec{r},\vec{p})=P(x,y,z,p_x,p_y,p_z)
$$


所以我們得到每個狀態的能量

$$
E_s=\frac{p_x^2+p_y^2+p_z^2}{2m}=\frac{p^2}{2m}
$$

所有的狀態積分會是

$$
\Sigma_s=\Sigma_{\vec{r}}\Sigma_{\vec{p}}=\int dx \int dy \int dz \int dp_x \int dp_y \int dp_z
$$


這樣就能回來算分割函數

$$
\begin{align}
Z &= \Sigma_s e^{\frac{-E_s}{kT}}\\
&= \Sigma_{\vec{r}}\Sigma_{\vec{p}} e^{-\frac{1}{2mkT}(p_x^2+p_y^2+p_z^2)}\\
&= (\int dx dy dz)(\int_{-\infty}^{\infty} dp_x e^{-\frac{p_x^2}{2mkT}})(\int_{-\infty}^{\infty} dp_y e^{-\frac{p_y^2}{2mkT}})(\int_{-\infty}^{\infty} dp_z e^{-\frac{p_z^2}{2mkT}})\\
&=V(2\pi mkT)^{\frac{1}{2}}(2\pi mkT)^{\frac{1}{2}}(2\pi mkT)^{\frac{1}{2}}\\
&(\because \int_{-\infty}^{\infty}dz e^{-z^2}=\sqrt{\pi})\\
&=V(2\pi mkT)^{\frac{3}{2}}\propto VT^{\frac{3}{2}}
\end{align}
$$


### 套用回Information Entropy

接著把這個理想氣體的波茲曼分布套回到Shannon Entropy

$$
\begin{align}
\sigma&=\langle - \ln P \rangle = \langle -\ln \left( \frac{1}{Z}e^{\frac{-p^2}{2mkT}} \right)\rangle\\
&=\ln Z + \frac{1}{kT}\langle \frac{p^2}{2m}\rangle
\end{align}
$$

從之前溫度指標的定義

$$
\bigg\langle \frac{1}{2}m(v_x^2+v_y^2+v_z^2)\bigg\rangle=\frac{3}{2}kT
$$

得到

$$
\begin{align}
\sigma &= \ln Z + \frac{3}{2}\\
&=\ln V + \frac{3}{2} \ln T +\sigma_0
\end{align}
$$

這就是對單一粒子的Shannon Entropy，後面常數項其實有普朗克常數，詳情可看Sockur-Terode Equation。


因為是理想氣體，所以整個系統假設有N個粒子，彼此互相獨立

$$
\sigma_N=N\cdot \sigma = N \ln V + \frac{3}{2} N \ln T + const
$$


我們用之前簡單的cycle來看

{% include posts-fig.html path="/physics_thermal/Fig6-3.png" %}

$$
\sigma_N(B)-\sigma_N(A)\\
= \left( N \ln V_B + \frac{3}{2} N \ln T_B + const\right)\\
- \left( N \ln V_A + \frac{3}{2} N \ln T_A + const\right)\\
=N\ln\left(\frac{V_B}{V_A}\right)+\frac{3}{2}N \ln \left(\frac{T_B}{T_A}\right)\\
=\frac{5}{2} N \ln 2
$$


一樣取這段算Thermal Entropy

$$
S(B)-S(A)\\
=nC_p \ln 2\\
=\frac{5}{2}nR\ln 2\\
=\frac{5}{2}Nk\ln 2
$$


兩者比較就得證至少對理想氣體來說

$$
S=k\sigma 
$$


### 小結思路

之前我們用一個簡單的square cycle，知道理想氣體積一圈，Thermal Entropy沒變，然後也可以取各自一段來算Entropy的差。


現在直接單純看粒子本身，我們不知道粒子在哪裡，所以用機率分布去描述他，看他在哪裡$\vec{r}$、並用多快$\vec{p}$的方式出現，且這個機率分布滿足波茲曼分布，算出來之後，就可以把這個機率分布帶進Information Entropy，算出來之後就可以針對每一點Entropy算差。


結果發現，這樣的方式去算每一點的Shannon Entropy的差，和之前用吸熱放熱的Thermal Entropy的差，居然是一樣的，只是中間差了一個波茲曼常數$k$。

不同角度出發得到一樣的結果，代表我們用熱去理解Entropy本身有更深刻的意義，等下會看到熱這件事情和機率分布有什麼關聯！
